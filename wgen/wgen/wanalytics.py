#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
wanalytics

Produce standard validation and calibration analytics on synthetic weather generated by
my stochastic weather generator, wgen.py. Assumes standard file and directory structure.

8 Mar 2019
Updated to remove all seasonal calculations, for simiplicity. Bug fixed in
load_weather() - line 59

10 Oct 2018
Updated to handle stochastic weather data in Parquet format

1 Nov 2018
Substantial redesign for speed optimisation and useability. Now read historic and
stochastic data from distinct sources, both in Parquet format. For historic weather,
it is now assumed that the user will use the 'by qid' weather in 'standard' format.

2 Jan 2019
Change to accomodate file formats generated by wgen2.

Mutahar Chalmers, RMS, 2018-2019
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import scipy.stats as st


def load_weather(sttgs, qid, inpath, hist=True, detrend=True):
    """Load daily weather data."""

    variables, asttgs, ssttgs = sttgs['variables'], sttgs['analysis'], sttgs['synthesis']

    # Load data by qid, including only simulated variables
    wdata = pd.read_parquet(inpath+'/{}.parquet'.format(qid))[variables]

    if (hist and detrend):
        # Year delta between pivot year and historic years
        years = (asttgs['pivot_year'] - wdata.index.get_level_values('year'))
        years = years.to_series(index=wdata.index)

        # Load pre-calculated trend data, select qid, reformat and apply linear trend
        for v in variables:
            trend = pd.read_parquet(asttgs['trend_files'][v]).loc[qid]#.rename_axis('month')
            trend.index = trend.index.rename('month').astype(int)
            trend_delta = years.mul(trend, level='month')
            wdata[v] = wdata[v] + trend_delta

    # Aggregate daily data to monthly
    wdata_monthly_agg = wdata.mean(level=['year','month'])
    for v in variables:
        if sttgs['agg'][v] == 'sum':
            wdata_monthly_agg[v] = wdata[v].sum(level=['year','month'])
    wdata_monthly_max = wdata.max(level=['year','month'])
    wdata_monthly_min = wdata.min(level=['year','month'])
    wdata_monthly_var = wdata.var(level=['year','month'])

    return wdata_monthly_agg, wdata_monthly_max, wdata_monthly_min, wdata_monthly_var


def load_climate(sttgs, hist=True):
    """Load daily weather and monthly climate data."""

    variables, asttgs, ssttgs = sttgs['variables'], sttgs['analysis'], sttgs['synthesis']

    # Load historic climate indices and combine with weather data
    cfile = asttgs['clim_hist'] if hist else ssttgs['clim_stoc']
    return pd.read_csv(cfile, index_col=[0,1])[asttgs['cixs']]


def climquant(wdata, qid):
    """
    Calculate climatology and quantiles of input weather data at monthly resolution.
    """

    clima_m, quant_m = {}, {}

    # Calculate climatology and quantiles to RP 1000 (annual (non-)EP = 0.001)
    ps = [0.1, 0.2, 0.5] + [p for p in range(1, 100, 1)] + [99.5, 99.8, 99.9]

    # Restrict quantiles to those empirically calculatable from data
    years = wdata.index.get_level_values('year')
    n_years = years.max() - years.min() + 1
    ps = [p for p in ps if (p>100/n_years) and (p<100-100/n_years)]

    # Scale percentiles correctly for pandas
    ps = [p/100 for p in ps]

    # Monthly and seasonal climatologies
    clima_m = wdata.mean(level='month').stack()
    clima_m.name = qid

    # Monthly quantiles
    for m in range(1, 13):
        for p in ps:
            quant_m[m, p] = wdata.xs(m, level='month').quantile(p)

    quant_m = pd.concat(quant_m, names=['month', 'quantile'])
    quant_m.name = qid
    return clima_m, quant_m


def dailyquant(wdata, qid):
    """
    Calculate daily quantiles on a monthly basis.
    """

    # Calculate climatology and quantiles to (non-)EP = 0.0001
    ps = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5] + [p for p in range(1, 100, 1)] + [99.5, 99.8, 99.9, 99.995, 99.998, 99.999]

    # Restrict quantiles to those empirically calculatable from data
    n_days = wdata.groupby(level='month').count().max().max()
    ps = [p for p in ps if (p>100/n_days) and (p<100-100/n_days)]

    # Scale percentiles correctly for pandas
    ps = [p/100 for p in ps]

    # Calculate quantiles for all variables
    qs = pd.concat({p: wdata.groupby(level='month').quantile(p) for p in ps},
                   names=['quantile'])
    return qs.swaplevel().sort_index().stack().rename(qid)


def similarity_KS(wdata_hist, wdata_stoc, qid):
    """Calculate Kolmogorov-Smirnov statistic and p-value to measure similarity."""

    ks_stat_m, ks_pval_m = {}, {}

    # Loop over months, then weather variables
    for m in range(1, 13):
        for v in wdata_stoc.columns:
            ks_stat_m[m, v], ks_pval_m[m, v] = st.ks_2samp(wdata_hist[v].xs(m, level='month'),
                                                           wdata_stoc[v].xs(m, level='month'))
    return pd.Series(ks_pval_m, name=qid)


def xcorr(wdata, qid):
    """Cross-correlation of weather variables and climate indices."""

    xcorr_m = {}

    # Monthly
    for m in range(1, 13):
        xcorr_m[m]= wdata.xs(m, level='month').corr()

    xcorr_m = pd.concat(xcorr_m).stack()
    xcorr_m.name = qid
    return xcorr_m


def scorr(monthly, sttgs, qid, qids_ref, inpath, hist=True):
    """Spatial-correlation of weather variables. Handles both hist and stoc."""

    corr = {}

    # Loop over reference qids
    for qid_ref in qids_ref:
        try:
            if hist:
                # NB - Only do spatial correlation on monthly sums/means
                monthly_ref, _, _, _ = load_weather(sttgs, qid_ref, inpath)
            else:
                # NB - Only do spatial correlation on monthly sums/means
                monthly_ref, _, _, _ = load_weather(sttgs, qid_ref, inpath, False)
            corr[qid_ref] = monthly.corrwith(monthly_ref)
        except:
            continue
    return pd.concat(corr).rename(qid)


def run(sttgs, qid, stoc_inpath, outpath):
    """Master function to run all other functions for a given qid."""

    # Path to historic daily weather data by qid
    hist_inpath = sttgs['synthesis']['histdailypath']

    # Kolmogorov-Smirnov test p-values, including for monthly max, min and variance
    print('  Kolmogorov-Smirnov tests...')
    ks_m = similarity_KS(hist_monthly, stoc_monthly, qid)
    ksmax_m = similarity_KS(hist_monthly_max, stoc_monthly_max, qid)
    ksmin_m = similarity_KS(hist_monthly_min, stoc_monthly_min, qid)
    ksvar_m = similarity_KS(hist_monthly_var, stoc_monthly_var, qid)

    # Spatial correlations
    qids_ref = sttgs['analytics']['qids_ref']
    if qid in qids_ref:
        print('Spatial correlations...')
        hist_scorr = scorr(hist_monthly_agg, sttgs, qid, qids_ref, hist_inpath)
        stoc_scorr = scorr(stoc_monthly_agg, sttgs, qid, qids_ref, stoc_inpath, hist=False)

    # Write historic analytics, storing as float32 to save disk space
    if qid in qids_ref:
        stoc_scorr.astype(np.float32).to_hdf(outpath+'/{0}.h5'.format(qid), 'stoc_scorr_m')
    else:
        pd.Series().to_hdf(outpath+'/{0}.h5'.format(qid), 'stoc_scorr_m')

    # Write K-S distribution statistics
    ks_m.astype(np.float32).to_hdf(outpath+'/{0}.h5'.format(qid), 'ks_m')
    ksmax_m.astype(np.float32).to_hdf(outpath+'/{0}.h5'.format(qid), 'ksmax_m')
    ksmin_m.astype(np.float32).to_hdf(outpath+'/{0}.h5'.format(qid), 'ksmin_m')
    ksvar_m.astype(np.float32).to_hdf(outpath+'/{0}.h5'.format(qid), 'ksvar_m')

    print('Complete.')

